{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crrVc69pAHO"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xqTpLAipAHV"
      },
      "source": [
        "Download docker and open cmd as administrator.\n",
        "下载docker，然后以管理员身份打开cmd。\n",
        "\n",
        "Download apache/spark-py by typing the following command in cmd (or just search for it in the search bar to download it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Db4w7K4DpAHX"
      },
      "outputs": [],
      "source": [
        "docker pull apache/spark-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XBLI2MUpAHZ"
      },
      "source": [
        "# Cluster setup and connectivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV0ZHU_pAHa"
      },
      "source": [
        "## Open docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "uJJUJrwJpAHa"
      },
      "outputs": [],
      "source": [
        "docker run -it apache/spark-py /opt/spark/bin/pyspark\n",
        "\n",
        "# If this step doesn't work you can try to open it manually in docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY8vLe-LpAHb"
      },
      "source": [
        "## Build and connect master and worker nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TrUo9S9npAHc"
      },
      "outputs": [],
      "source": [
        "# master：\n",
        "docker run -it -u root -p 8080:8080 --name spark_master apache/spark-py /bin/bash\n",
        "cd ..\n",
        "./bin/spark-class org.apache.spark.deploy.master.Master\n",
        "\n",
        "# worker：\n",
        "# If you want to create more than one worker node just set a new name and repeat the code.\n",
        "docker run -it -u root --link spark_master:spark_master --name spark_worker apache/spark-py /bin/bash\n",
        "cd ..\n",
        "./bin/spark-class org.apache.spark.deploy.worker.Worker spark://172.17.0.2:7077\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGcemexpAHd"
      },
      "source": [
        "# Connecting with VSCode and Writing & running code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaY9oagfpAHd"
      },
      "source": [
        "## Connecting with VSCode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPExWAJApAHe"
      },
      "source": [
        "In this step, simply tap on VSCode and follow the default guide to download all the required extensions, such as the Remote series and python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqqcfjd1pAHf"
      },
      "source": [
        "## Writing & running code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ0ihgXjpAHf"
      },
      "source": [
        "Use VSCode to open the remote control and create a py file under the work_dir path where you can write python code directly. Then use the code below to import the py file written into the master node and run it using spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "LM4AxShrpAHg"
      },
      "outputs": [],
      "source": [
        "\n",
        "/opt/spark/bin/spark-submit --master spark://172.17.0.2:7077 /opt/spark/work-dir/test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-4g_AwpAHg"
      },
      "source": [
        "We give two sample test py-files below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "08guEb-EpAHh"
      },
      "outputs": [],
      "source": [
        "# Spark Call Test\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark Test\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a sample dataset\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "random.seed(1)  # Setting the random seed\n",
        "\n",
        "names = [''.join(random.choices(string.ascii_uppercase, k=5)) for _ in range(1000000)]\n",
        "ages = [random.randint(20, 60) for _ in range(1000000)]\n",
        "\n",
        "data = list(zip(names, ages))\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Printing dataset contents\n",
        "df.show()\n",
        "\n",
        "# Counting people older than 30\n",
        "count = df.filter(df.Age > 30).count()\n",
        "print(\"Number of people with age greater than 30:\", count)\n",
        "\n",
        "# Close SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NsvPGbaWpAHh"
      },
      "outputs": [],
      "source": [
        "# Online connection test\n",
        "import requests\n",
        "\n",
        "def get_data_from_api(api_url):\n",
        "    response = requests.get(api_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "api_url = 'https://jsonplaceholder.typicode.com/posts'  # Here's a sample API\n",
        "data = get_data_from_api(api_url)\n",
        "\n",
        "if data is not None:\n",
        "    print('Cool')\n",
        "else:\n",
        "    print('Failed to get data from API')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU8C6Lt8pAHj"
      },
      "source": [
        "Now you can view your output in the results bar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JbE03BipAHj"
      },
      "source": [
        "# Restart the cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2LdLfrWpAHk"
      },
      "source": [
        "Reconnecting the nodes is required after restarting the cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIo8Z342pAHk"
      },
      "source": [
        "# Open the nodes in docker and coding in CMD as administrator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Master:\n",
        "docker exec -it spark_master bash\n",
        "cd ..\n",
        "./bin/spark-class org.apache.spark.deploy.master.Master\n",
        "\n",
        "Worker:\n",
        "docker exec -it spark_worker bash\n",
        "cd ..\n",
        "./bin/spark-class org.apache.spark.deploy.worker.Worker spark://172.17.0.2:7077"
      ],
      "metadata": {
        "id": "BA2LAOXApLMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtaYrLg9pAHl"
      },
      "source": [
        "And then open VSCode to connect to the cluster and utilize the former run-py command to run the py file."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}